{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bc9e87-8130-4822-b991-12329c215cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from binance.um_futures import UMFutures\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# --- API Configuration ---\n",
    "# IMPORTANT: Replace with your own API keys\n",
    "api_key = \"YOUR_API_KEY\"\n",
    "api_secret = \"YOUR_SECRET_KEY\"\n",
    "\n",
    "# Initialize Futures client\n",
    "client = UMFutures(key=api_key, secret=api_secret)\n",
    "\n",
    "# --- Parameters ---\n",
    "symbol = \"ETHUSDT\"\n",
    "start_date = \"2024-08-10\"\n",
    "interval = \"1d\"  # Data interval (e.g., \"15m\", \"1h\", \"1d\")\n",
    "\n",
    "print(f\"Fetching {symbol} Futures kline data for {interval} interval...\")\n",
    "\n",
    "# Convert start date to timestamp\n",
    "start_time = int(pd.Timestamp(start_date).timestamp() * 1000)\n",
    "end_time = int(pd.Timestamp.now().timestamp() * 1000)\n",
    "\n",
    "# List to hold all klines\n",
    "all_klines = []\n",
    "\n",
    "# Loop to fetch all data in batches\n",
    "current_time = start_time\n",
    "batch_count = 0\n",
    "\n",
    "while current_time < end_time:\n",
    "    batch_count += 1\n",
    "    print(f\"Fetching batch {batch_count}... (Starting from: {pd.to_datetime(current_time, unit='ms')})\")\n",
    "    \n",
    "    # Fetch 1500 klines (Binance's max limit)\n",
    "    klines = client.klines(\n",
    "        symbol=symbol,\n",
    "        interval=interval,\n",
    "        startTime=current_time,\n",
    "        limit=1500\n",
    "    )\n",
    "    \n",
    "    if not klines:\n",
    "        print(\"No more data received, ending loop.\")\n",
    "        break\n",
    "    \n",
    "    # Add to the list\n",
    "    all_klines.extend(klines)\n",
    "    \n",
    "    # Update time for the next batch (1ms after the last kline's open time)\n",
    "    # Using [0] (Open time) ensures we don't skip data\n",
    "    current_time = klines[-1][0] + 1\n",
    "    \n",
    "    # Brief sleep to respect rate limits\n",
    "    time.sleep(0.1)\n",
    "\n",
    "print(f\"Total {len(all_klines)} records fetched.\")\n",
    "\n",
    "# --- Data Processing ---\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(all_klines, columns=[\n",
    "    'timestamp', 'open', 'high', 'low', 'close', 'volume', \n",
    "    'close_time', 'quote_asset_volume', 'number_of_trades', \n",
    "    'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'\n",
    "])\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "\n",
    "# Remove duplicate records just in case\n",
    "df = df.drop_duplicates(subset=['timestamp'])\n",
    "\n",
    "# Convert numeric columns to float\n",
    "numeric_cols = ['open', 'high', 'low', 'close', 'volume', 'taker_buy_base_asset_volume', 'quote_asset_volume']\n",
    "df[numeric_cols] = df[numeric_cols].astype(float)\n",
    "\n",
    "# Select only desired columns\n",
    "df_final = df[['timestamp', 'open', 'high', 'low', 'close', 'volume', 'taker_buy_base_asset_volume', 'quote_asset_volume']].copy()\n",
    "\n",
    "# Rename Taker Buy Volume column\n",
    "df_final.rename(columns={'taker_buy_base_asset_volume': 'taker_buy_volume'}, inplace=True)\n",
    "\n",
    "# Sort by date\n",
    "df_final = df_final.sort_values('timestamp')\n",
    "\n",
    "# --- DATA QUALITY & MISSING DATA CHECK ---\n",
    "print(\"\\n=== DATA QUALITY CHECK ===\")\n",
    "\n",
    "# Calculate the expected time range\n",
    "expected_start = pd.Timestamp(start_date).tz_localize(None) # Remove timezone info for comparison\n",
    "expected_end = pd.Timestamp.now().floor(interval)     # Floor to the last completed interval\n",
    "\n",
    "# Generate all expected timestamps based on the 'interval' variable\n",
    "try:\n",
    "    expected_timestamps = pd.date_range(\n",
    "        start=expected_start, \n",
    "        end=expected_end, \n",
    "        freq=interval\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"Error generating date range: {e}. Check if 'interval' is a valid pandas frequency.\")\n",
    "    expected_timestamps = pd.DatetimeIndex([])\n",
    "\n",
    "\n",
    "print(f\"Expected time range: {expected_start} - {expected_end}\")\n",
    "print(f\"Expected total records: {len(expected_timestamps)}\")\n",
    "print(f\"Fetched (unique) records: {len(df_final)}\")\n",
    "\n",
    "# Find missing timestamps\n",
    "if not expected_timestamps.empty:\n",
    "    actual_timestamps = set(df_final['timestamp'])\n",
    "    expected_timestamps_set = set(expected_timestamps)\n",
    "    missing_timestamps = sorted(expected_timestamps_set - actual_timestamps)\n",
    "    \n",
    "    if missing_timestamps:\n",
    "        print(f\"\\nüö® {len(missing_timestamps)} MISSING candlesticks detected!\")\n",
    "        print(\"\\nFirst 10 missing timestamps:\")\n",
    "        for i, missing in enumerate(missing_timestamps[:10]):\n",
    "            print(f\"  {i+1}. {missing}\")\n",
    "        \n",
    "        if len(missing_timestamps) > 10:\n",
    "            print(f\"  ... and {len(missing_timestamps) - 10} more\")\n",
    "        \n",
    "        # Save missing timestamps to a CSV\n",
    "        missing_df = pd.DataFrame({'missing_timestamp': missing_timestamps})\n",
    "        missing_file = f\"{symbol}_FUTURES_{interval.upper()}_MISSING.csv\"\n",
    "        missing_df.to_csv(missing_file, index=False)\n",
    "        print(f\"\\nMissing timestamps saved to: {missing_file}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚úÖ No missing candlestick data found! All data is complete.\")\n",
    "\n",
    "    # --- Consecutive Gap Analysis ---\n",
    "    print(\"\\n=== CONSECUTIVE GAP ANALYSIS ===\")\n",
    "    \n",
    "    if missing_timestamps:\n",
    "        gaps = []\n",
    "        current_gap_start = missing_timestamps[0]\n",
    "        current_gap_end = missing_timestamps[0]\n",
    "        \n",
    "        # Create a Timedelta from the interval for comparison\n",
    "        interval_delta = pd.to_timedelta(interval)\n",
    "        \n",
    "        for i in range(1, len(missing_timestamps)):\n",
    "            # Check if the next missing timestamp is exactly one interval after the previous one\n",
    "            if missing_timestamps[i] == missing_timestamps[i-1] + interval_delta:\n",
    "                current_gap_end = missing_timestamps[i]\n",
    "            else:\n",
    "                # The gap has ended, record it\n",
    "                gaps.append((current_gap_start, current_gap_end))\n",
    "                # Start a new gap\n",
    "                current_gap_start = missing_timestamps[i]\n",
    "                current_gap_end = missing_timestamps[i]\n",
    "        \n",
    "        # Add the last gap\n",
    "        gaps.append((current_gap_start, current_gap_end))\n",
    "        \n",
    "        print(f\"Found {len(gaps)} total consecutive gap(s):\")\n",
    "        for i, (start, end) in enumerate(gaps[:5]): # Show first 5 gaps\n",
    "            duration = (end - start)\n",
    "            if start == end:\n",
    "                print(f\"  {i+1}. {start} (1 candlestick)\")\n",
    "            else:\n",
    "                print(f\"  {i+1}. {start} to {end} (Duration: {duration})\")\n",
    "        \n",
    "        if len(gaps) > 5:\n",
    "            print(f\"  ... and {len(gaps) - 5} more gaps\")\n",
    "\n",
    "    # --- Data Quality Summary ---\n",
    "    print(\"\\n=== DATA QUALITY SUMMARY ===\")\n",
    "    if len(expected_timestamps) > 0:\n",
    "        completeness = (len(df_final) / len(expected_timestamps)) * 100\n",
    "        print(f\"Data completeness: {completeness:.2f}%\")\n",
    "        \n",
    "        if completeness >= 99:\n",
    "            print(\"‚úÖ Excellent data quality!\")\n",
    "        elif completeness >= 95:\n",
    "            print(\"üü° Good data quality\")\n",
    "        elif completeness >= 90:\n",
    "            print(\"üü† Medium data quality\")\n",
    "        else:\n",
    "            print(\"üî¥ Low data quality - use with caution!\")\n",
    "    else:\n",
    "        print(\"Could not calculate completeness (no expected timestamps).\")\n",
    "\n",
    "# Duplicate check (should be 0 after drop_duplicates)\n",
    "duplicates = df_final.duplicated(subset=['timestamp']).sum()\n",
    "if duplicates > 0:\n",
    "    print(f\"üö® {duplicates} duplicate records found (this shouldn't happen).\")\n",
    "else:\n",
    "    print(\"‚úÖ No duplicate records.\")\n",
    "\n",
    "# --- Save to CSV ---\n",
    "output_file = f\"{symbol}_FUTURES_{interval.upper()}_DATA.csv\"\n",
    "df_final.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\nüìÅ Main data saved to: {output_file} ({len(df_final)} records)\")\n",
    "print(f\"üìä Columns: {list(df_final.columns)}\")\n",
    "\n",
    "# Final data range info\n",
    "if not df_final.empty:\n",
    "    print(f\"üìÖ Data range: {df_final['timestamp'].min()} to {df_final['timestamp'].max()}\")\n",
    "\n",
    "print(\"\\n=== LAST 5 RECORDS ===\")\n",
    "print(df_final.tail())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
